---
title: "Utilisation SVM pour analyse de données"
subtitle: "Mme BADEL Anne"
author: "Eric"
date: "4 Juin 2019"
output:
  revealjs::revealjs_presentation:
    center: yes
    highlight: kate
    theme: solarized
    transition: fade
  ioslides_presentation:
    highlight: kate

---

```{r, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, comment = NA)
```

```{r, message= 'hide'}
library("ggplot2")
library("lattice")
library("revealjs")
library("caret")
library("e1071")
#library ("ggvis")
library("GGally")
library("RColorBrewer")
```

```{r}
setwd(".")
donnee = read.table("./../data/placenta90.208.Rdata.txt", header = T, dec = ".", sep = "\t")
set.seed(1000)
```

```{r fonctions}
tx.bon <- function(table) {
  return(sum(diag(table)) / sum(table))
}

tx.erreur <- function(table) {
  acc <- (sum(diag(table)) / sum(table))
  return(1 - acc)
}

sensibilite <- function(table) {
  if (nrow(table) >= 2) {
    return(table[1,1] / (table[1,1] + table[2,1]))
  } else {
    return(NA)
  }
}

specificite <- function(table) {
  if (nrow(table) >= 2) {
    return(table[2,2] / (table[2,2] + table[1,2]))
  } else {
    return(NA)
  }
}
```
***

But de l'étude:

On souhaite établir un modèle de prédiction de la clairance à partir de descripteurs géométrique et physico-chimiques de petits composés. Nous allons utiliser la méthode SVM pour répondre à la question.

##Preparation du jeu de données

***
Présentation des données
```{r resume.donnee}
#summary(donnee)
head(donnee[,c(1:4)])
col_donnee = ncol(donnee)
names(col_donnee) = "Nombre de descripteurs"
row_donnee = nrow(donnee)
names(row_donnee) = "Nombre de molecules (médicaments)"
col_donnee
row_donnee
```

```{r CI_class}
donnee$CI_class = ifelse(donnee$CI <= 0.5, "petit","grand")
donnee.fin = donnee[,-1]
donnee.fin$CI_class = as.factor(donnee.fin$CI_class)
```

***
Normalisation des données

```{r normalisation}
donnee.fin2 = scale(donnee.fin[,-208], center = TRUE, scale = TRUE)
boxplot(donnee.fin2[,c(0:25)], las = 2)
#La variance est entre -4 et 4 mais en réalité c'est -10 a 10 environs
donnee.fin2 = data.frame(donnee.fin2,donnee.fin$CI_class)
```

***
Visualisation des données grâce à une remise à l'échelle des donnée.(MDS)
```{r MDS, fig.align = "center"}
donnee.cmd = cmdscale(dist(donnee.fin2))
plot(donnee.cmd, col = donnee.fin$CI_class)
```

##Création des échantillons d'apprentissage et de test

***
Création d'un échantillon d'apprentissage
```{r}
ech.App = sample(1:dim(donnee.fin2)[1],2*dim(donnee.fin2)[1]/3)
mat.App = donnee.fin2[ech.App,-c(168,188)]
head(mat.App[,c(1:4)])
matApp_row = nrow(mat.App)
names(matApp_row) = "individus dans échantillon d'apprentissage"
matApp_row
```

***
Création d'un échantillon de validation (test)
```{r}
ech.Test = setdiff(1:nrow(donnee.fin2),ech.App)
mat.Test = donnee.fin2[ech.Test,]
head(mat.Test[,c(1:4)])
matTest_row = nrow(mat.Test)
names(matTest_row) = "individus dans l'échantillon test"
matTest_row
```

***
Verification de la répartition des individus dans nos échantillons (test d'homogénéité)
```{r homogeneite, fig.align= "center"}
par(mfrow=c(1,2))
boxplot (mat.App[,c(10:20)], las = 2)
boxplot (mat.Test[,c(10:20)], las = 2)
```

***
Résultats des prédictions avec différents noyaux (kernel)

```{r}
knitr::include_graphics("Kernel.png")
```

***
Conclusion

On conclu que le meilleur modèle est le modele utilisant le kernel linéaire

```{r acp, fig.align = "center", messages = 'hide'}
#par(mfrow = c(1,2))
donnee.acp <- prcomp(as.matrix(donnee.fin[,-208]))
plot(donnee.acp)
#biplot(donnee.acp)
```
